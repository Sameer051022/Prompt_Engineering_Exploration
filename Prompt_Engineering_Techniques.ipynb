{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6cafb53c",
      "metadata": {
        "id": "6cafb53c"
      },
      "source": [
        "# Exploring Prompt Engineering Techniques with Google Gemini\n",
        "\n",
        "## Objective\n",
        "\n",
        "This will help you understand and implement various prompt engineering techniques using Google Gemini.\n",
        "You will explore different types of prompts to see how they influence the behavior and responses of a language model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca91ef65",
      "metadata": {
        "id": "ca91ef65"
      },
      "source": [
        "## Task 1: Setup and Basic Interaction\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "- Connect to Google Gemini using the API key.\n",
        "- Create a basic prompt and receive a response from the model.\n",
        "- Analyze the response quality and relevance.\n",
        "\n",
        "### Code Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "44cf34cb-b53b-4dc6-8526-d221687c358c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "44cf34cb-b53b-4dc6-8526-d221687c358c",
        "outputId": "6b5ceaf1-6c63-4561-85fd-b29c26bbc1a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI works by mimicking human intelligence processes through machines, especially computer systems.  It's a broad field encompassing many approaches, but most AI systems rely on some combination of the following:\n",
            "\n",
            "**1. Data:**  AI systems learn from data. The more data they are trained on, the better they generally perform. This data can be structured (like data in a spreadsheet) or unstructured (like text, images, or audio).\n",
            "\n",
            "**2. Algorithms:**  These are sets of rules and statistical techniques that allow the AI system to learn from the data.  Different algorithms are suited to different tasks.  Some common types include:\n",
            "\n",
            "* **Machine Learning (ML):**  This is a subset of AI where systems learn from data without explicit programming.  Instead of being explicitly programmed with rules, they identify patterns and relationships in the data to make predictions or decisions.  ML further breaks down into categories like:\n",
            "    * **Supervised Learning:** The algorithm is trained on a labeled dataset (data with known inputs and outputs), learning to map inputs to outputs. Example: image classification (training an AI to identify cats in images).\n",
            "    * **Unsupervised Learning:** The algorithm is trained on an unlabeled dataset, identifying patterns and structures in the data without pre-defined categories. Example: customer segmentation (grouping customers based on their purchasing behavior).\n",
            "    * **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards for desirable actions and penalties for undesirable actions. Example: training a robot to navigate a maze.\n",
            "\n",
            "* **Deep Learning (DL):** This is a subfield of ML that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data.  These networks are inspired by the structure and function of the human brain.  Deep learning excels at tasks involving complex patterns, such as image recognition, natural language processing, and speech recognition.\n",
            "\n",
            "**3. Models:**  The algorithm processes the data and creates a model. This model is a representation of the patterns and relationships learned from the data.  The model can then be used to make predictions or decisions on new, unseen data.\n",
            "\n",
            "**4. Inference (Prediction/Decision-Making):** Once a model is trained, it can be used to make inferences on new data. This is the process of using the learned model to predict outcomes or make decisions.\n",
            "\n",
            "**In Simple Terms:** Imagine teaching a child to identify a cat. You'd show them many pictures of cats (data), pointing out their features (algorithm).  Over time, the child learns to recognize cats (model) and can identify a cat in a new picture (inference). AI works similarly, but at a much larger scale and with more sophisticated algorithms.\n",
            "\n",
            "**Limitations:**  While AI has made tremendous progress, it's important to remember its limitations:\n",
            "\n",
            "* **Bias:** AI systems can inherit biases present in the data they are trained on, leading to unfair or discriminatory outcomes.\n",
            "* **Lack of Common Sense:** AI systems often struggle with tasks that require common sense or understanding of the world.\n",
            "* **Explainability:**  It can be difficult to understand how some AI systems arrive at their decisions, making it hard to debug or trust their outputs (\"black box\" problem).\n",
            "* **Data Dependency:**  AI systems require large amounts of data to function effectively.\n",
            "\n",
            "\n",
            "This is a simplified explanation, and the specific techniques and approaches used in AI are constantly evolving. However, it provides a general understanding of the fundamental principles.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"Add Your Api Key Here\")\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\"Explain how AI works\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68d4d00",
      "metadata": {
        "id": "f68d4d00"
      },
      "source": [
        "## Task 2: Exploring Different Prompt Types\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "- **Zero-shot Prompt:** Create a prompt where the model is asked to perform a task without prior examples.\n",
        "- **One-shot Prompt:** Provide a single example to guide the model on the expected task.\n",
        "- **Few-shot Prompt:** Use several examples to shape the model's responses more effectively.\n",
        "\n",
        "### Code Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "h62nj8vSgj-S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "h62nj8vSgj-S",
        "outputId": "7c0be747-2c93-47d0-8239-388569e05b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Prompt engineering is incredibly significant in AI, particularly in the realm of large language models (LLMs) and other generative AI systems, because it directly impacts the quality, relevance, and usefulness of the output.  Essentially, it's the art and science of crafting effective prompts that elicit the desired response from the AI.  Its significance stems from several key factors:\\n\\n* **Bridging the Communication Gap:** LLMs are powerful but lack inherent understanding of context or intent.  Prompt engineering serves as the bridge, translating human requests into a format the AI can process and understand.  A poorly crafted prompt can lead to irrelevant, nonsensical, or even harmful outputs, while a well-crafted prompt unlocks the model's full potential.\\n\\n* **Controlling Output Quality:**  The quality of the AI's response \\u2013 be it a text, image, code, or other data \\u2013 is heavily dependent on the prompt.  Through careful wording, structure, and the inclusion of specific instructions (e.g., \\\"write a short story in the style of Edgar Allan Poe\\\"), prompt engineers can steer the AI towards generating high-quality, creative, and accurate results.\\n\\n* **Enhancing Efficiency and Productivity:**  Effectively prompting an AI can significantly reduce the time and effort required to achieve a specific goal.  Instead of manually performing tasks, users can leverage the AI's capabilities by providing clear instructions, thereby automating processes and boosting productivity.\\n\\n* **Unlocking Specific Capabilities:** LLMs are multi-faceted tools.  Prompt engineering allows users to tap into specific capabilities of the model.  For example, a prompt can instruct the AI to translate languages, summarize text, generate different creative formats (poems, scripts, code), or even act as a conversational chatbot with a specific persona.\\n\\n* **Mitigating Risks:**  Harmful or biased outputs from AI are often a consequence of poorly designed prompts.  Prompt engineering plays a critical role in mitigating these risks by incorporating safety guidelines, constraints, and ethical considerations into the prompt design.  This helps ensure that the AI generates outputs that are aligned with human values and avoid producing harmful or biased content.\\n\\n* **Driving Innovation:** As AI models become more sophisticated, prompt engineering techniques will continue to evolve, unlocking new capabilities and applications.  This iterative process of refining prompt engineering techniques will be crucial in pushing the boundaries of what's possible with AI.\\n\\n\\nIn short, prompt engineering is not merely a technical skill; it's a crucial bridge between human intention and AI capability.  Its importance will only grow as AI systems become more pervasive and integral to various aspects of life.\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.38108432741093456\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 10,\n",
            "        \"candidates_token_count\": 532,\n",
            "        \"total_token_count\": 542\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot Prompt Example\n",
        "# Example (this is a placeholder, replace with actual code):\n",
        "prompt = 'Explain the significance of prompt engineering in AI.'\n",
        "response = model.generate_content(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "vUvtqSxQlY3X",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "vUvtqSxQlY3X",
        "outputId": "c53ab714-6947-430c-82fe-4241f82d6897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Berlin\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -4.8394747864222154e-05\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 22,\n",
            "        \"candidates_token_count\": 2,\n",
            "        \"total_token_count\": 24\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# One-shot Prompt Example\n",
        "# Example (this is a placeholder, replace with actual code):\n",
        "prompt = 'Example: What is the capital of France?\\nAnswer: Paris\\n\\nWhat is the capital of Germany?'\n",
        "response = model.generate_content(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "RNGKZ445l1di",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "RNGKZ445l1di",
        "outputId": "2e38b00a-c6f8-4ae3-eded-449ffc37f9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Prompt engineering in AI is significant because it bridges the gap between human intention and AI capabilities.  While AI models are powerful, they are fundamentally tools that require careful direction.  Prompt engineering is the art and science of crafting effective input prompts to guide AI models, such as large language models (LLMs), to generate desired outputs.  Its significance lies in several key areas:\\n\\n* **Controlling Output Quality and Relevance:**  A poorly crafted prompt can lead to irrelevant, nonsensical, or biased outputs.  Effective prompt engineering ensures the AI focuses on the specific task and produces high-quality, accurate results.  This involves careful consideration of keywords, context, constraints, and desired format.\\n\\n* **Unlocking Model Potential:** AI models are capable of impressive feats, but they need clear instructions. Prompt engineering acts as this instruction manual, allowing users to tap into the full potential of the model for tasks like text generation, translation, summarization, question answering, and code generation.  Sophisticated prompting techniques can even elicit creative or unexpected outputs.\\n\\n* **Bias Mitigation:**  AI models are trained on data, and biases present in this data can be reflected in the model's outputs.  Careful prompt engineering can help mitigate these biases by explicitly stating desired values or avoiding potentially triggering phrases.\\n\\n* **Improving Efficiency and Cost-Effectiveness:**  Well-crafted prompts reduce the need for iterative refinement and multiple attempts.  This saves time, resources, and computational power, making the use of AI more efficient and cost-effective.\\n\\n* **Accessibility and Democratization:**  Prompt engineering lowers the barrier to entry for interacting with powerful AI models. By making it easier to obtain desirable results, it makes AI more accessible to a wider range of users, even those without deep technical expertise.\\n\\n\\nIn short, prompt engineering is not just a technical skill; it's a crucial element in harnessing the power of AI responsibly and effectively.  It's an evolving field, with new techniques and best practices constantly emerging as our understanding of AI models deepens.\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.3584719437819261\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 122,\n",
            "        \"candidates_token_count\": 416,\n",
            "        \"total_token_count\": 538\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Few-shot Prompt Example\n",
        "# Example (this is a placeholder, replace with actual code):\n",
        "prompts = [\n",
        "    \"Question: What is AI? Answer: Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\",\n",
        "    \"Question: What is machine learning? Answer: Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.\",\n",
        "    \"Question: What is deep learning? Answer: Deep learning is a subset of machine learning in machine intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled.\"\n",
        "]\n",
        "\n",
        "final_question = \"Question: Explain the significance of prompt engineering in AI.\"\n",
        "\n",
        "# Combine all examples and the final question into one prompt\n",
        "combined_prompt = \"\\n\".join(prompts) + \"\\n\" + final_question\n",
        "\n",
        "# Assuming 'model' is already defined and can generate responses based on the provided prompt\n",
        "response = model.generate_content(combined_prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b231edc",
      "metadata": {
        "id": "2b231edc"
      },
      "source": [
        "## Task 3: Controlled Prompts\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "- Implement a task using controlled prompts that direct the model to use a specific style or tone.\n",
        "- Experiment with prompts that constrain the model to generate content within a specific domain knowledge.\n",
        "\n",
        "### Code Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ba21885e-88b9-4ab3-8aeb-04ad6d0933cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "ba21885e-88b9-4ab3-8aeb-04ad6d0933cd",
        "outputId": "8a7d09f0-caef-4edb-aa9a-ece29679c453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Data security is like a really good joke \\u2013 if everyone knows the punchline (your password), it's not funny anymore.  And unlike a bad joke, a bad data breach can leave you with more than just a cringe; it can leave you with identity theft, a hefty bill, and a profound sense of existential dread about the fragility of the digital age. So, please, for the love of all that is holy and encrypted, choose strong passwords! (And maybe don't use \\\"password123\\\" \\u2013 even if it *is* easy to remember).\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.3895790294065314\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 22,\n",
            "        \"candidates_token_count\": 118,\n",
            "        \"total_token_count\": 140\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Controlled Prompt Example with Manual Style Application\n",
        "prompt = 'Write a humorous comment on the importance of data security. Perhaps, include a joke about passwords or encryption.'\n",
        "response = model.generate_content(prompt)  # Adjusting the prompt to encourage a humorous response\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5c1941",
      "metadata": {
        "id": "7f5c1941"
      },
      "source": [
        "## Task 4: Chain-of-Thought Prompting\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "- Use chain-of-thought prompting to solve a complex problem step-by-step.\n",
        "- Evaluate how effectively the model can handle logical reasoning through this technique.\n",
        "\n",
        "### Code Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c46fee1b-9fa5-4daf-a187-cacace767f41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "c46fee1b-9fa5-4daf-a187-cacace767f41",
        "outputId": "bae2ca81-a6af-4039-9239-4f1ae0726604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Let's apply the three-step process to solving global warming:\\n\\n**Step 1: Understand the problem:**\\n\\nGlobal warming, or climate change, is the long-term heating of Earth's climate system observed since the pre-industrial period (between 1850 and 1900) due to human activities, primarily fossil fuel burning, which increases heat-trapping greenhouse gas levels in Earth's atmosphere. The consequences are widespread and severe, including rising global temperatures, more frequent and intense heatwaves, changes in precipitation patterns (including more droughts and floods), melting glaciers and polar ice, rising sea levels, ocean acidification, and disruptions to ecosystems and biodiversity.  The problem is complex, involving interconnected physical, biological, economic, social, and political systems.  It's not just about rising temperatures; it's about the cascading effects these temperature increases have on the entire planet.\\n\\n**Step 2: Gather data:**\\n\\nThis step requires a massive amount of data collection across multiple disciplines. We need:\\n\\n* **Climate data:**  Temperature records from various sources (satellites, weather stations, ice cores), greenhouse gas concentrations (atmospheric monitoring), sea level rise data, glacier and ice sheet mass balance, ocean temperatures and salinity, precipitation patterns.\\n* **Emissions data:** Data on greenhouse gas emissions from various sources (energy production, transportation, industry, agriculture, deforestation) at national and global levels. This includes both historical data and projections of future emissions based on various scenarios.\\n* **Socioeconomic data:** Population growth projections, economic activity, energy consumption patterns, technological advancements, land use changes, and policy decisions at national and international levels.\\n* **Impact data:**  Information on the observed and projected impacts of climate change, such as changes in agricultural yields, sea level rise affecting coastal communities, increased frequency and intensity of extreme weather events, and biodiversity loss.\\n\\nThis data needs to be analyzed using sophisticated climate models to understand the complex interactions and predict future climate scenarios.\\n\\n**Step 3: Solve the problem:**\\n\\nSolving global warming requires a multi-pronged approach focusing on mitigation (reducing greenhouse gas emissions) and adaptation (adjusting to the already unavoidable impacts of climate change).  Key strategies include:\\n\\n* **Mitigation:**\\n    * **Transition to renewable energy:**  Rapidly scaling up renewable energy sources like solar, wind, geothermal, and hydropower, coupled with improvements in energy storage technologies.\\n    * **Energy efficiency improvements:** Reducing energy consumption through building retrofits, more efficient transportation systems, and industrial process improvements.\\n    * **Carbon capture and storage:** Developing and deploying technologies to capture CO2 emissions from power plants and industrial sources and store them underground.\\n    * **Sustainable transportation:**  Promoting electric vehicles, public transportation, cycling, and walking, as well as improving fuel efficiency of conventional vehicles.\\n    * **Sustainable land use and forestry:**  Protecting and restoring forests, promoting sustainable agriculture practices that reduce emissions (e.g., reducing deforestation, improving soil health), and improving land management to enhance carbon sequestration.\\n    * **Policy and regulations:** Implementing carbon pricing mechanisms (e.g., carbon taxes, cap-and-trade systems), investing in research and development of clean technologies, and setting ambitious emission reduction targets.\\n\\n* **Adaptation:**\\n    * **Infrastructure improvements:**  Building more resilient infrastructure (e.g., seawalls, flood defenses) to protect against the impacts of climate change.\\n    * **Water management:**  Improving water resource management to cope with changing precipitation patterns and increasing water scarcity.\\n    * **Disaster preparedness:**  Strengthening disaster preparedness and response systems to deal with more frequent and intense extreme weather events.\\n    * **Agricultural adaptation:**  Developing climate-resilient crops and farming practices to maintain food security.\\n    * **Community-based adaptation:**  Empowering communities to adapt to the impacts of climate change based on their specific needs and vulnerabilities.\\n\\n\\nSolving global warming is a global challenge requiring international cooperation, significant investment, and widespread societal changes.  No single solution exists, but a combination of the strategies outlined above, driven by strong political will, technological innovation, and public engagement, is essential to address this critical issue.\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"citation_metadata\": {\n",
            "            \"citation_sources\": [\n",
            "              {\n",
            "                \"start_index\": 152,\n",
            "                \"end_index\": 302,\n",
            "                \"uri\": \"https://www.insightsonindia.com/2020/11/02/secure-synopsis-29-september-2020/\"\n",
            "              },\n",
            "              {\n",
            "                \"start_index\": 186,\n",
            "                \"end_index\": 380,\n",
            "                \"uri\": \"https://f1000research.com/articles/12-1074\"\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          \"avg_logprobs\": -0.24649467203963987\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 31,\n",
            "        \"candidates_token_count\": 866,\n",
            "        \"total_token_count\": 897\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Chain-of-Thought Prompting Example\n",
        "# Example (this is a placeholder, replace with actual code):\n",
        "prompt = 'Step 1: Understand the problem\\nStep 2: Gather data\\nStep 3: Solve the problem\\n\\nHow would you solve global warming?'\n",
        "response = model.generate_content(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a438a059",
      "metadata": {
        "id": "a438a059"
      },
      "source": [
        "## Task 5: Analysis and Reflection\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "- Compare the responses across different prompt types and discuss the impact of prompt design on the model's output.\n",
        "- Reflect on the limitations and potential biases introduced by different prompting strategies.\n",
        "\n",
        "### Reflective Questions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "vwecmK4QtwIK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vwecmK4QtwIK",
        "outputId": "7bbe2d27-abf8-4890-d065-26565707fe56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected responses for analysis:\n",
            "zero_shot: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Prompt engineering is incredibly significant in AI, particularly in the realm of large language models (LLMs) like GPT-3, LaMDA, and others.  Its significance stems from the fact that it directly impacts the quality, relevance, and usefulness of the AI's output.  Here's a breakdown:\\n\\n* **Bridging the Gap Between Human Intention and Machine Understanding:** LLMs are powerful, but they don't inherently understand the nuances of human language and intent.  A well-crafted prompt acts as a translator, clearly specifying the desired task, context, and format to the AI.  A poorly written prompt, on the other hand, can lead to inaccurate, irrelevant, or nonsensical responses.\\n\\n* **Controlling the AI's Behavior and Output:**  Through prompt engineering, we can guide the AI towards generating specific types of text, like creative writing, code, summaries, translations, etc.  We can also control the style, tone, and length of the output by carefully structuring the prompt.  This control is crucial for various applications, from generating marketing copy to creating personalized educational materials.\\n\\n* **Improving Model Performance and Efficiency:**  A well-engineered prompt can significantly reduce the number of iterations needed to obtain the desired output.  This saves time and resources, especially when dealing with complex tasks.  Furthermore, skillful prompt engineering can sometimes elicit better results than simply using a more powerful or larger model.\\n\\n* **Unlocking Creative Potential and Novel Applications:** Prompt engineering is not just about getting factual information; it's about unleashing the AI's creative capabilities. By crafting imaginative and nuanced prompts, we can generate surprising and innovative content that was previously unimaginable. This opens doors for new applications in areas like art generation, storytelling, and game design.\\n\\n* **Mitigation of Bias and Harmful Outputs:**  Prompt engineering plays a vital role in mitigating biases present in LLMs. By carefully designing prompts, we can minimize the likelihood of the AI generating biased, offensive, or harmful content.  This is a crucial aspect of responsible AI development and deployment.\\n\\n* **Expanding Accessibility and Usability:**  Effective prompt engineering makes AI more accessible to a wider audience.  Users without deep technical expertise can effectively interact with sophisticated LLMs by using carefully designed prompts, unlocking the power of AI for a broader range of applications.\\n\\n\\nIn essence, prompt engineering is not just a technical skill but a crucial element in shaping the future of AI.  It's the art and science of effectively communicating with AI systems to unlock their full potential and ensure responsible and beneficial utilization.  As AI models become increasingly sophisticated, the importance of prompt engineering will only continue to grow.\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.39769043869637916\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 10,\n",
            "        \"candidates_token_count\": 542,\n",
            "        \"total_token_count\": 552\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n",
            "\n",
            "one_shot: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"ML stands for Machine Learning.\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.00019456343060093268\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 18,\n",
            "        \"candidates_token_count\": 7,\n",
            "        \"total_token_count\": 25\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n",
            "\n",
            "few_shot: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence \\\"deep\\\") to analyze data and extract higher-level features.  It's capable of learning complex patterns and representations from unstructured or unlabeled data, unlike more traditional machine learning algorithms which often require more structured data and feature engineering.\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.15589300791422525\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 44,\n",
            "        \"candidates_token_count\": 66,\n",
            "        \"total_token_count\": 110\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have functions that generate responses based on different prompts\n",
        "def get_response(prompt):\n",
        "    # This function is a placeholder for the actual model.generate(prompt) method\n",
        "    return model.generate_content(prompt)\n",
        "\n",
        "# Collecting responses from different prompt types\n",
        "responses = {\n",
        "    'zero_shot': get_response(\"Explain the significance of prompt engineering in AI.\"),\n",
        "    'one_shot': get_response(\"Example: What is AI? AI stands for Artificial Intelligence.\\nWhat is ML?\"),\n",
        "    'few_shot': get_response(\"Example: What is AI? AI is the simulation of human intelligence by machines.\\n\"\n",
        "                             \"Example: What is ML? ML is a subset of AI that focuses on machine learning from data.\\n\"\n",
        "                             \"What is deep learning?\")\n",
        "}\n",
        "\n",
        "print(\"Collected responses for analysis:\")\n",
        "for key, value in responses.items():\n",
        "    print(f\"{key}: {value}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "GovA4lAMwWKR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GovA4lAMwWKR",
        "outputId": "5b9b862d-2860-4f3e-8e35-6f69ac1c18c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analysis of zero_shot Response:\n",
            "Length of response: 89 characters\n",
            "Sentiment scores: {'neg': 0.0, 'neu': 0.78, 'pos': 0.22, 'compound': 0.4767}\n",
            "Flesch-Kincaid Grade Level: 8.20\n",
            "Keyword presence: {'security': False, 'data': False, 'privacy': False}\n",
            "\n",
            "Analysis of one_shot Response:\n",
            "Length of response: 81 characters\n",
            "Sentiment scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Flesch-Kincaid Grade Level: 13.90\n",
            "Keyword presence: {'security': False, 'data': False, 'privacy': False}\n",
            "\n",
            "Analysis of few_shot Response:\n",
            "Length of response: 95 characters\n",
            "Sentiment scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Flesch-Kincaid Grade Level: 9.10\n",
            "Keyword presence: {'security': False, 'data': True, 'privacy': False}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import textstat\n",
        "\n",
        "# Ensure necessary NLTK components are downloaded\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_responses(responses):\n",
        "    for key, response in responses.items():\n",
        "        response_text = response if isinstance(response, str) else str(response)\n",
        "\n",
        "        # Length of response\n",
        "        length = len(response_text)\n",
        "\n",
        "        # Sentiment analysis\n",
        "        sentiment = sia.polarity_scores(response_text)\n",
        "\n",
        "        # Readability score using textstat\n",
        "        fk_score = textstat.flesch_kincaid_grade(response_text)\n",
        "\n",
        "        # Print analysis results\n",
        "        print(f\"\\nAnalysis of {key} Response:\")\n",
        "        print(f\"Length of response: {length} characters\")\n",
        "        print(f\"Sentiment scores: {sentiment}\")\n",
        "        print(f\"Flesch-Kincaid Grade Level: {fk_score:.2f}\")\n",
        "\n",
        "        # Example of keyword analysis\n",
        "        keywords = ['security', 'data', 'privacy']\n",
        "        keyword_presence = {word: word in response_text for word in keywords}\n",
        "        print(\"Keyword presence:\", keyword_presence)\n",
        "\n",
        "# Example usage\n",
        "analyze_responses({\n",
        "    'zero_shot': \"AI works by simulating human intelligence processes through machines. It's a broad field.\",\n",
        "    'one_shot': \"AI is the simulation of human processes by machines, especially computer systems.\",\n",
        "    'few_shot': \"AI systems learn to perform tasks by processing large amounts of data and recognizing patterns.\"\n",
        "})\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
